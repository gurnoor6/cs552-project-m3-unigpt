{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "krKf7BGT9zHV"
      },
      "source": [
        "## Setup: Imports and Installations"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "LN73txlR0ei4"
      },
      "source": [
        "following the tutorial from :https://docs.argilla.io/en/latest/guides/llms/examples/train-reward-model-rlhf.html"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "aRiW8Bwn4OjI"
      },
      "source": [
        "# Part 1 With noraml weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6L4cSSK9zHX"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install pandas\n",
        "!pip install torch\n",
        "!pip install importlib\n",
        "\n",
        "#preprocessing\n",
        "!pip install trl peft\n",
        "!pip install -q -U git+https://github.com/lvwerra/trl.git git+https://github.com/huggingface/transformers.git git+https://github.com/huggingface/accelerate.git git+https://github.com/huggingface/peft.git\n",
        "!pip install -q datasets bitsandbytes einops\n",
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ65r2Xb9zHY",
        "outputId": "74120957-3954-43c1-8de7-d7265fedad92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('http'), PosixPath('//172.28.0.1'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-a100-s-21jdi2yilnod5 --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//ipykernel.pylab.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from datasets import Dataset, DatasetDict\n",
        "from google.colab import drive\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_ as clip_grad_norm\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from tqdm import tqdm\n",
        "from transformers import (AdamW, AutoModel, AutoModelForSequenceClassification, AutoConfig,\n",
        "                          AutoTokenizer, Trainer, TrainingArguments, get_polynomial_decay_schedule_with_warmup,\n",
        "                          get_linear_schedule_with_warmup)\n",
        "from transformers import DistilBertModel, DistilBertTokenizer, TrainingArguments\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import Dataset, load_dataset\n",
        "from trl import RewardTrainer\n",
        "from peft import LoraConfig, TaskType, prepare_model_for_kbit_training, PeftModel, PromptLearningConfig,MODEL_TYPE_TO_PEFT_MODEL_MAPPING,get_peft_model\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/MyDrive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_l37SyYq0nk"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bIXpgKSCiSwo"
      },
      "source": [
        "# 1 GPT2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsfwHf7tiSLl",
        "outputId": "0039cee2-eb27-41ea-e0ab-a2233ef531ae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(42044293120, 42481549312)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.mem_get_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y3XC5So6arJr"
      },
      "outputs": [],
      "source": [
        "with open('training_dataset.json',encoding=\"cp437\", errors='ignore') as json_file:\n",
        "    train = json.load(json_file)\n",
        "\n",
        "max_tokens = 512\n",
        "\n",
        "full = pd.DataFrame(train)\n",
        "\n",
        "full = full[(full['length_0']<=max_tokens)]\n",
        "\n",
        "train, test = train_test_split(full, test_size=0.1, random_state=18625541)\n",
        "\n",
        "train = pd.DataFrame(train)\n",
        "train = train[(train['length_0']<=max_tokens)&(train['length_1']<=max_tokens)]\n",
        "list_of_dicts = train.to_dict(orient='records')\n",
        "train = Dataset.from_list(list_of_dicts)\n",
        "\n",
        "test = pd.DataFrame(test)\n",
        "test = test[(test['length_0']<=max_tokens)&(test['length_1']<=max_tokens)]\n",
        "list_of_dicts = test.to_dict(orient='records')\n",
        "test = Dataset.from_list(list_of_dicts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73,
          "referenced_widgets": [
            "81935f7f67e14a699bdd1801c4062d06",
            "9a2594b088ff4ab89a1abe607b5a63c7",
            "7b8fddabfec64b518082c6f3b5247502",
            "bdc831ab4a8d42d2b1f2d320f63e6cf9",
            "cd2cbe2de51b40299c1a8830a7018756",
            "f82cb297757048d4aa24cfad2b56ba7f",
            "eb75a735050e477f8d13d70c12adb7f8",
            "68dc60728f114f63a2947544f30ef6b5",
            "4e66fbc2b4214a8698ccbd9a0ae4319b",
            "d9ca1130102543af99bf727245d36f6e",
            "0402d6de68554f109dc3d65e19c04b99",
            "88a5d7780d9242f1bfcd3aa01a7cdf9b",
            "0d9b0a83816a4fa6b0c1836aae949154",
            "3cd7427d9942487880d36812919c6498",
            "89861bc0db3a4d75bea11dc09a706e00",
            "a3eb6ad44b9f4777ba40ba9b318a43dd",
            "5934c316e22a467995ace2070ff8c111",
            "f00ee97110ef4dd19c87d725864208dc",
            "85fdf22d367847f0b2217f4c17f1ecae",
            "4cb90234c1d44482a7542b030a630093",
            "be99cfca4780466d81d7543efb817d73",
            "15e3d6222ff84981b8c903f8f7a32e69"
          ]
        },
        "id": "dljKlRo0iX8y",
        "outputId": "0778d50b-8d99-4f25-b7e2-dcb7d36dba04"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81935f7f67e14a699bdd1801c4062d06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/303 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88a5d7780d9242f1bfcd3aa01a7cdf9b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2716 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = \"gpt2-medium\"\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "def formatting_func(examples):\n",
        "    kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": max_tokens, \"return_tensors\": \"pt\"}\n",
        "\n",
        "    # Prepend the prompt and a line break to the original_response and response-1 fields.\n",
        "    prompt_plus_chosen_response = examples[\"chat_1\"]\n",
        "    prompt_plus_rejected_response = examples[\"chat_0\"]\n",
        "\n",
        "    # Then tokenize these modified fields.\n",
        "    tokens_chosen = tokenizer.encode_plus(prompt_plus_chosen_response, **kwargs)\n",
        "    tokens_rejected = tokenizer.encode_plus(prompt_plus_rejected_response, **kwargs)\n",
        "\n",
        "    return {\n",
        "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0], \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
        "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0], \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n",
        "    }\n",
        "\n",
        "test = test.map(formatting_func)\n",
        "train = train.map(formatting_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "EKjfHikJiZOf",
        "outputId": "0efc4c6c-f017-49aa-a20d-b55590b34bba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 354,824,192 || all params: 354,824,192 || trainable%: 100.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/reward_trainer.py:136: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2716' max='2716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2716/2716 36:23, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.811200</td>\n",
              "      <td>0.459826</td>\n",
              "      <td>0.801980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.424900</td>\n",
              "      <td>0.482120</td>\n",
              "      <td>0.858086</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.517100</td>\n",
              "      <td>0.457892</td>\n",
              "      <td>0.805281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.472600</td>\n",
              "      <td>0.530502</td>\n",
              "      <td>0.825083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.432400</td>\n",
              "      <td>0.516286</td>\n",
              "      <td>0.828383</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.382500</td>\n",
              "      <td>0.601951</td>\n",
              "      <td>0.851485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.339500</td>\n",
              "      <td>0.369525</td>\n",
              "      <td>0.861386</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.412700</td>\n",
              "      <td>0.592931</td>\n",
              "      <td>0.877888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.330800</td>\n",
              "      <td>0.623947</td>\n",
              "      <td>0.877888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.202200</td>\n",
              "      <td>0.569762</td>\n",
              "      <td>0.891089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.209400</td>\n",
              "      <td>0.519859</td>\n",
              "      <td>0.877888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.195800</td>\n",
              "      <td>0.415014</td>\n",
              "      <td>0.891089</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.193700</td>\n",
              "      <td>0.431427</td>\n",
              "      <td>0.897690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.100000</td>\n",
              "      <td>0.526089</td>\n",
              "      <td>0.894389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.089900</td>\n",
              "      <td>0.490052</td>\n",
              "      <td>0.900990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.027100</td>\n",
              "      <td>0.534651</td>\n",
              "      <td>0.897690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.086200</td>\n",
              "      <td>0.532053</td>\n",
              "      <td>0.900990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2700</td>\n",
              "      <td>0.049200</td>\n",
              "      <td>0.550926</td>\n",
              "      <td>0.900990</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2716, training_loss=0.2916414544798313, metrics={'train_runtime': 2184.1674, 'train_samples_per_second': 4.974, 'train_steps_per_second': 1.243, 'total_flos': 0.0, 'train_loss': 0.2916414544798313, 'epoch': 4.0})"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print_trainable_parameters(model)\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./reward_model\",\n",
        "    per_device_train_batch_size=4,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=300,\n",
        "    logging_steps=150,\n",
        "    num_train_epochs=4,\n",
        "\n",
        ")\n",
        "trainer = RewardTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=test,\n",
        "    max_length=max_tokens\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHWwX49mMGB3",
        "outputId": "a30b9ca8-f12b-4ee0-ec05-3be1c497ff26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13586\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([ 1.7503318 ,  9.521019  , -1.9541194 , ..., -7.261831  ,\n",
              "       -0.56273746, -2.6336484 ], dtype=float32)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Flatten the list of batched outputs\n",
        "with open('eval_dataset.json',encoding=\"cp437\", errors='ignore') as json_file:\n",
        "    eval = json.load(json_file)\n",
        "\n",
        "max_tokens = 512\n",
        "\n",
        "eval = pd.DataFrame(eval)\n",
        "list_of_dicts = eval.to_dict(orient='records')\n",
        "eval = Dataset.from_list(list_of_dicts)\n",
        "\n",
        "eval_encoded = eval.map(lambda examples: tokenizer(examples['chat'], truncation=True, padding='longest',max_length=max_tokens), batched=True)\n",
        "\n",
        "print_trainable_parameters(model)\n",
        "\n",
        "eval_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "dataloader = DataLoader(eval_encoded, batch_size=1,num_workers=0)\n",
        "\n",
        "# Disable gradient calculation\n",
        "outputs = []\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(dataloader):\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Move batch to device\n",
        "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
        "        # Get model's output\n",
        "        output = model(**batch)\n",
        "        logits = output.logits.detach().cpu().numpy()\n",
        "        outputs.append(logits)\n",
        "flattened_outputs = np.concatenate([batch[0] for batch in outputs], axis=0)\n",
        "\n",
        "print(len(flattened_outputs))\n",
        "flattened_outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mw0xY3MKFyW"
      },
      "outputs": [],
      "source": [
        "eval_df = pd.DataFrame(list_of_dicts)\n",
        "eval_df['predictions'] = flattened_outputs.tolist()\n",
        "eval_df.to_json(\"GPT2.json\", orient=\"records\", indent=4)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "eJXUtPW2Kb9G"
      },
      "source": [
        "# 2 ELECTRA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SwzSTDCrbA9",
        "outputId": "e5cd0479-c15c-484b-b0d4-89155f72105a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(31451578368, 42481549312)"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.mem_get_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3_J5DDXrbpf"
      },
      "outputs": [],
      "source": [
        "with open('trainin_dataset.json',encoding=\"cp437\", errors='ignore') as json_file:\n",
        "    train = json.load(json_file)\n",
        "\n",
        "max_tokens = 512\n",
        "\n",
        "full = pd.DataFrame(train)\n",
        "\n",
        "full = full[(full['length_0']<=max_tokens)]\n",
        "\n",
        "train, test = train_test_split(full, test_size=0.1, random_state=18625541)\n",
        "\n",
        "train = pd.DataFrame(train)\n",
        "train = train[(train['length_0']<=max_tokens)&(train['length_1']<=max_tokens)]\n",
        "list_of_dicts = train.to_dict(orient='records')\n",
        "train = Dataset.from_list(list_of_dicts)\n",
        "\n",
        "test = pd.DataFrame(test)\n",
        "test = test[(test['length_0']<=max_tokens)&(test['length_1']<=max_tokens)]\n",
        "list_of_dicts = test.to_dict(orient='records')\n",
        "test = Dataset.from_list(list_of_dicts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "7ffa82004db140f692c01965532f19b3",
            "362ab3fa194b4d1e8b96c37994a6b2cd",
            "61c53c66b1e84922982125745e847904",
            "8c5c395c57124de5b79fb38f3779483f",
            "90c7355e2cc143f89543b93792d07151",
            "a70c0d52fb0a4489b55bac7aabaa46cb",
            "d543dba028c6436db9b2dd179b05a10f",
            "b146db5409e44b29a14da22b2a036a54",
            "6910f14f801e40a0bb9388128a6dc059",
            "479dfdbe1ff746aca5bf9c91d322a7d4",
            "2db6c5e732764aa88fca6a626c92e779",
            "4cb341a7a3934f2ebda6ac8232d02451",
            "27b84f164d224576b39569b8e173efd4",
            "5d89d8982b434ef58bd3d924c1ceeb96",
            "838b2d52e34d47c6beaf672b03a47d81",
            "2232a4410850476abf0091ff2a9fb5ac",
            "05b4f0c8c4694c78af264eeba1c9a9d2",
            "8e658aa0ccb44bf6979266f0c8410847",
            "8633274057b3478e93904f761eafa08d",
            "5efd8fa19c8c4bd18190c831d15681e3",
            "f2199ef658b74e6f84b6dbba73bfc74f",
            "21c7d64071f34c83a0c5f025dbcc52cf"
          ]
        },
        "id": "Ecu5e5oVrYeF",
        "outputId": "3d96e44d-78e6-414c-9719-6029f98e4202"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ffa82004db140f692c01965532f19b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/498 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cb341a7a3934f2ebda6ac8232d02451",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1970 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = \"OpenAssistant/reward-model-electra-large-discriminator\"\n",
        "\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.add_special_tokens({'eos_token': '[PAD]'})\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "def formatting_func(examples):\n",
        "    kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": max_tokens, \"return_tensors\": \"pt\"}\n",
        "\n",
        "    # Prepend the prompt and a line break to the original_response and response-1 fields.\n",
        "    prompt_plus_chosen_response = examples[\"chat_1\"]\n",
        "    prompt_plus_rejected_response = examples[\"chat_0\"]\n",
        "\n",
        "    # Then tokenize these modified fields.\n",
        "    tokens_chosen = tokenizer.encode_plus(prompt_plus_chosen_response, **kwargs)\n",
        "    tokens_rejected = tokenizer.encode_plus(prompt_plus_rejected_response, **kwargs)\n",
        "\n",
        "    return {\n",
        "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0], \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
        "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0], \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n",
        "    }\n",
        "\n",
        "test = test.map(formatting_func)\n",
        "train = train.map(formatting_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n2Pb9_W1rqtT",
        "outputId": "febc7708-dc9a-4c69-fb92-fdc8305e1357"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 8,392,706 || all params: 342,484,994 || trainable%: 2.4505324750082336\n"
          ]
        }
      ],
      "source": [
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        },
        "id": "MCx35M-ZrpB8",
        "outputId": "5ffe6419-8bfa-42a7-caca-590df66ead79"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/reward_trainer.py:136: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1972' max='1972' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1972/1972 19:43, Epoch 4/4]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.604000</td>\n",
              "      <td>0.597138</td>\n",
              "      <td>0.710843</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.598100</td>\n",
              "      <td>0.541728</td>\n",
              "      <td>0.738956</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.504100</td>\n",
              "      <td>0.507564</td>\n",
              "      <td>0.748996</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.511300</td>\n",
              "      <td>0.480599</td>\n",
              "      <td>0.761044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.485200</td>\n",
              "      <td>0.447678</td>\n",
              "      <td>0.769076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.408600</td>\n",
              "      <td>0.474170</td>\n",
              "      <td>0.775100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.498000</td>\n",
              "      <td>0.445216</td>\n",
              "      <td>0.771084</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.373100</td>\n",
              "      <td>0.449525</td>\n",
              "      <td>0.785141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.415200</td>\n",
              "      <td>0.447581</td>\n",
              "      <td>0.783133</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1972, training_loss=0.4828656295250202, metrics={'train_runtime': 1183.6072, 'train_samples_per_second': 6.658, 'train_steps_per_second': 1.666, 'total_flos': 0.0, 'train_loss': 0.4828656295250202, 'epoch': 4.0})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./reward_model_electra\",\n",
        "    per_device_train_batch_size=4,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=500,\n",
        "    logging_steps=200,\n",
        "    num_train_epochs=4,\n",
        ")\n",
        "trainer = RewardTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=test,\n",
        "    max_length=max_tokens\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "cojfRmItsAz2"
      },
      "source": [
        "# 3 ROBERTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCD4IgMJsHzo",
        "outputId": "9b4a8eac-dade-4849-e998-da8a7da21dc8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(42044293120, 42481549312)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.mem_get_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xvQDaDHsmPQ"
      },
      "outputs": [],
      "source": [
        "with open('trainin_dataset.json',encoding=\"cp437\", errors='ignore') as json_file:\n",
        "    train = json.load(json_file)\n",
        "\n",
        "max_tokens = 512\n",
        "\n",
        "full = pd.DataFrame(train)\n",
        "\n",
        "full = full[(full['length_0']<=max_tokens)]\n",
        "\n",
        "train, test = train_test_split(full, test_size=0.1, random_state=18625541)\n",
        "\n",
        "train = pd.DataFrame(train)\n",
        "train = train[(train['length_0']<=max_tokens)&(train['length_1']<=max_tokens)]\n",
        "list_of_dicts = train.to_dict(orient='records')\n",
        "train = Dataset.from_list(list_of_dicts)\n",
        "\n",
        "test = pd.DataFrame(test)\n",
        "test = test[(test['length_0']<=max_tokens)&(test['length_1']<=max_tokens)]\n",
        "list_of_dicts = test.to_dict(orient='records')\n",
        "test = Dataset.from_list(list_of_dicts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "e619868d514c4bf4a33f7cb96e13e707",
            "83bab447f258418b8a795d41f507bbea",
            "cb7e1f8ece62420680a4ebab2a92afd5",
            "31d7cff0b77a407c8b91edbcfb0ab7f8",
            "be82587ef1e14137b671a5b6e8b709c5",
            "c84ff8b11ecc48fab046a053072ee766",
            "dd4ad12037b744a4aa524c848e1bc7f0",
            "1ded6961e5634cd3b845f7f916dd299f",
            "551a4623c15c403bb36ad0a9ba131535",
            "28eb21e19aae489aaaf9de952c551f21",
            "c87ad2ecbd5e48bdb515c52f9a53c885",
            "37cc4039832248b18962523ebd67fb8a",
            "d44a288f184641f08a6b9b788e92f85b",
            "20c774598fb848868d7064367dd4742c",
            "15bb07895cb4418dbdb5d78de92fb345",
            "45ef6ba576984912ac8d650e72944cbd",
            "e4275c30d66f4e6f93fcb22ccb63858f",
            "1a920a25f6a04a15abee42958e5e0993",
            "e68cd4a5423d44429c1ebaaf6b872c9d",
            "d3a39d9de0cd4233865443fda32e8ad8",
            "5d0be9465d834418b0c092e68cb17e29",
            "72fc7945121745699e04d508a3649cd8"
          ]
        },
        "id": "eLGNYG84sltw",
        "outputId": "9c4d1b3a-37fe-4a9c-a49a-8684b10b0989"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e619868d514c4bf4a33f7cb96e13e707",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/498 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37cc4039832248b18962523ebd67fb8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1970 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "model_name = \"argilla/roberta-base-reward-model-falcon-dolly\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.add_special_tokens({'eos_token': '[PAD]'})\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "def formatting_func(examples):\n",
        "    kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": max_tokens, \"return_tensors\": \"pt\"}\n",
        "\n",
        "    # Prepend the prompt and a line break to the original_response and response-1 fields.\n",
        "    prompt_plus_chosen_response = examples[\"chat_1\"]\n",
        "    prompt_plus_rejected_response = examples[\"chat_0\"]\n",
        "\n",
        "    # Then tokenize these modified fields.\n",
        "    tokens_chosen = tokenizer.encode_plus(prompt_plus_chosen_response, **kwargs)\n",
        "    tokens_rejected = tokenizer.encode_plus(prompt_plus_rejected_response, **kwargs)\n",
        "\n",
        "    return {\n",
        "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0], \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
        "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0], \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n",
        "    }\n",
        "\n",
        "test = test.map(formatting_func)\n",
        "train = train.map(formatting_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQv-61eWsuYh",
        "outputId": "0454b450-8d23-4d88-e09d-cbb3beaad159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,362,370 || all params: 83,890,178 || trainable%: 2.8160269251067747\n"
          ]
        }
      ],
      "source": [
        "lora_alpha = 16\n",
        "lora_dropout = 0.1\n",
        "lora_r = 64\n",
        "\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=lora_alpha,\n",
        "    lora_dropout=lora_dropout,\n",
        "    r=lora_r,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        ")\n",
        "model = get_peft_model(model, peft_config)\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JIggpRuTsvXz",
        "outputId": "3a1be39a-b64a-47d7-dbe4-56796688260f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/reward_trainer.py:136: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;cell line: 21&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">21</span>                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1539</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1536 │   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1537 │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1538 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1539 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1540 │   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1541 │   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1542 │   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1801</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1798 │   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.control = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.callback_handler.on_step_begin(args, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state,  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1799 │   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1800 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accelerator.accumulate(model):                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1801 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1802 │   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1803 │   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1804 │   │   │   │   │   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2636</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2633 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss_mb.reduce_mean().detach().to(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.device)                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2634 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2635 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.compute_loss_context_manager():                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2636 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.compute_loss(model, inputs)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2637 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2638 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.n_gpu &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2639 │   │   │   </span>loss = loss.mean()  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># mean() to average on multi-gpu parallel training</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/trl/trainer/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">reward_trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">169</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">compute_loss</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">166 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">raise</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">NotImplementedError</span>(                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">167 │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"compute_loss is only implemented for RewardDataCollatorWithPadding, ple</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">168 │   │   │   </span>)                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>169 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>rewards_chosen = model(input_ids=inputs[<span style=\"color: #808000; text-decoration-color: #808000\">\"input_ids_chosen\"</span>], attention_mask=inpu   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">170 │   │   </span>rewards_rejected = model(                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">171 │   │   │   </span>input_ids=inputs[<span style=\"color: #808000; text-decoration-color: #808000\">\"input_ids_rejected\"</span>], attention_mask=inputs[<span style=\"color: #808000; text-decoration-color: #808000\">\"attention_mas</span>   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172 │   │   </span>)[<span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>]                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/peft/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">peft_model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">566</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 563 │   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 564 │   │   </span>peft_config = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.active_peft_config                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 565 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(peft_config, PromptLearningConfig):                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 566 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.base_model(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 567 │   │   │   │   </span>input_ids=input_ids,                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 568 │   │   │   │   </span>attention_mask=attention_mask,                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 569 │   │   │   │   </span>inputs_embeds=inputs_embeds,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/peft/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">peft_model.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">566</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 563 │   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 564 │   │   </span>peft_config = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.active_peft_config                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 565 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">isinstance</span>(peft_config, PromptLearningConfig):                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 566 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.base_model(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 567 │   │   │   │   </span>input_ids=input_ids,                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 568 │   │   │   │   </span>attention_mask=attention_mask,                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 569 │   │   │   │   </span>inputs_embeds=inputs_embeds,                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1218</span> in  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1215 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1216 │   │   </span>return_dict = return_dict <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> return_dict <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.config.use_return  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1217 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1218 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.roberta(                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1219 │   │   │   </span>input_ids,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1220 │   │   │   </span>attention_mask=attention_mask,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1221 │   │   │   </span>token_type_ids=token_type_ids,                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">852</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 849 │   │   │   </span>inputs_embeds=inputs_embeds,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 850 │   │   │   </span>past_key_values_length=past_key_values_length,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 851 │   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 852 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>encoder_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encoder(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 853 │   │   │   </span>embedding_output,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 854 │   │   │   </span>attention_mask=extended_attention_mask,                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 855 │   │   │   </span>head_mask=head_mask,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">527</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 524 │   │   │   │   │   </span>encoder_attention_mask,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 525 │   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 526 │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 527 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>layer_outputs = layer_module(                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 528 │   │   │   │   │   </span>hidden_states,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 529 │   │   │   │   │   </span>attention_mask,                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 530 │   │   │   │   │   </span>layer_head_mask,                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">411</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 408 │   </span>) -&gt; Tuple[torch.Tensor]:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 409 │   │   # decoder uni-directional self-attention cached key/values tuple is at positions</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 410 │   │   </span>self_attn_past_key_value = past_key_value[:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2</span>] <span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> past_key_value <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 411 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>self_attention_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.attention(                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 412 │   │   │   </span>hidden_states,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 413 │   │   │   </span>attention_mask,                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 414 │   │   │   </span>head_mask,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">338</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 335 │   │   </span>past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 336 │   │   </span>output_attentions: Optional[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">bool</span>] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 337 │   </span>) -&gt; Tuple[torch.Tensor]:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 338 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>self_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.self(                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 339 │   │   │   </span>hidden_states,                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 340 │   │   │   </span>attention_mask,                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 341 │   │   │   </span>head_mask,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_roberta.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">195</span> in   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 192 │   │   </span>past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>,                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 193 │   │   </span>output_attentions: Optional[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">bool</span>] = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">False</span>,                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 194 │   </span>) -&gt; Tuple[torch.Tensor]:                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 195 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>mixed_query_layer = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.query(hidden_states)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 196 │   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 197 │   │   # If this is instantiated as a cross-attention module, the keys</span>                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 198 │   │   # and values come from an encoder; the attention mask needs to be</span>                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 │   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 │   │   # Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 │   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 │   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/usr/local/lib/python3.10/dist-packages/peft/tuners/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">lora.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">630</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">627 │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.unmerge()                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">628 │   │   │   </span>result = F.linear(x, transpose(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fan_in_fan_out), bias=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">629 │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">elif</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.r[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.active_adapter] &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">and</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.merged:                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>630 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>result = F.linear(x, transpose(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.fan_in_fan_out), bias=<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">631 │   │   │   </span>                                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">632 │   │   │   </span>x = x.to(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.lora_A[<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.active_adapter].weight.dtype)                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">633 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
              "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
              "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">RuntimeError: </span>CUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">cublasCreate</span><span style=\"font-weight: bold\">(</span>handle<span style=\"font-weight: bold\">)</span>`\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
              "\u001b[31m│\u001b[0m in \u001b[92m<cell line: 21>\u001b[0m:\u001b[94m21\u001b[0m                                                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1539\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1536 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1537 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1538 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1539 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1540 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1541 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1542 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1801\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1798 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_step_begin(args, \u001b[96mself\u001b[0m.state,  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1799 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1800 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.accelerator.accumulate(model):                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1801 \u001b[2m│   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1802 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1803 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1804 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2636\u001b[0m in \u001b[92mtraining_step\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2633 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m loss_mb.reduce_mean().detach().to(\u001b[96mself\u001b[0m.args.device)                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2634 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2635 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.compute_loss_context_manager():                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2636 \u001b[2m│   │   │   \u001b[0mloss = \u001b[96mself\u001b[0m.compute_loss(model, inputs)                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2637 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2638 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.n_gpu > \u001b[94m1\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m2639 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = loss.mean()  \u001b[2m# mean() to average on multi-gpu parallel training\u001b[0m        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/trl/trainer/\u001b[0m\u001b[1;33mreward_trainer.py\u001b[0m:\u001b[94m169\u001b[0m in \u001b[92mcompute_loss\u001b[0m        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m166 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mNotImplementedError\u001b[0m(                                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m167 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mcompute_loss is only implemented for RewardDataCollatorWithPadding, ple\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m168 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m169 \u001b[2m│   │   \u001b[0mrewards_chosen = model(input_ids=inputs[\u001b[33m\"\u001b[0m\u001b[33minput_ids_chosen\u001b[0m\u001b[33m\"\u001b[0m], attention_mask=inpu   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m170 \u001b[0m\u001b[2m│   │   \u001b[0mrewards_rejected = model(                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids=inputs[\u001b[33m\"\u001b[0m\u001b[33minput_ids_rejected\u001b[0m\u001b[33m\"\u001b[0m], attention_mask=inputs[\u001b[33m\"\u001b[0m\u001b[33mattention_mas\u001b[0m   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   │   \u001b[0m)[\u001b[94m0\u001b[0m]                                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/peft/\u001b[0m\u001b[1;33mpeft_model.py\u001b[0m:\u001b[94m566\u001b[0m in \u001b[92mforward\u001b[0m                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 563 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 564 \u001b[0m\u001b[2m│   │   \u001b[0mpeft_config = \u001b[96mself\u001b[0m.active_peft_config                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 565 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(peft_config, PromptLearningConfig):                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 566 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.base_model(                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 567 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_ids=input_ids,                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 568 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mattention_mask=attention_mask,                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 569 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs_embeds=inputs_embeds,                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/peft/\u001b[0m\u001b[1;33mpeft_model.py\u001b[0m:\u001b[94m566\u001b[0m in \u001b[92mforward\u001b[0m                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 563 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 564 \u001b[0m\u001b[2m│   │   \u001b[0mpeft_config = \u001b[96mself\u001b[0m.active_peft_config                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 565 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m \u001b[96misinstance\u001b[0m(peft_config, PromptLearningConfig):                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 566 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m.base_model(                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 567 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minput_ids=input_ids,                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 568 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mattention_mask=attention_mask,                                            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 569 \u001b[0m\u001b[2m│   │   │   │   \u001b[0minputs_embeds=inputs_embeds,                                              \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/\u001b[0m\u001b[1;33mmodeling_roberta.py\u001b[0m:\u001b[94m1218\u001b[0m in  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1215 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1216 \u001b[0m\u001b[2m│   │   \u001b[0mreturn_dict = return_dict \u001b[94mif\u001b[0m return_dict \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m \u001b[96mself\u001b[0m.config.use_return  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1217 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1218 \u001b[2m│   │   \u001b[0moutputs = \u001b[96mself\u001b[0m.roberta(                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1219 \u001b[0m\u001b[2m│   │   │   \u001b[0minput_ids,                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1220 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=attention_mask,                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1221 \u001b[0m\u001b[2m│   │   │   \u001b[0mtoken_type_ids=token_type_ids,                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/\u001b[0m\u001b[1;33mmodeling_roberta.py\u001b[0m:\u001b[94m852\u001b[0m in   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 849 \u001b[0m\u001b[2m│   │   │   \u001b[0minputs_embeds=inputs_embeds,                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 850 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_values_length=past_key_values_length,                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 851 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 852 \u001b[2m│   │   \u001b[0mencoder_outputs = \u001b[96mself\u001b[0m.encoder(                                                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 853 \u001b[0m\u001b[2m│   │   │   \u001b[0membedding_output,                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 854 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=extended_attention_mask,                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 855 \u001b[0m\u001b[2m│   │   │   \u001b[0mhead_mask=head_mask,                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/\u001b[0m\u001b[1;33mmodeling_roberta.py\u001b[0m:\u001b[94m527\u001b[0m in   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 524 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mencoder_attention_mask,                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 525 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 526 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 527 \u001b[2m│   │   │   │   \u001b[0mlayer_outputs = layer_module(                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 528 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhidden_states,                                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 529 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mattention_mask,                                                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 530 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mlayer_head_mask,                                                      \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/\u001b[0m\u001b[1;33mmodeling_roberta.py\u001b[0m:\u001b[94m411\u001b[0m in   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 408 \u001b[0m\u001b[2m│   \u001b[0m) -> Tuple[torch.Tensor]:                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 409 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# decoder uni-directional self-attention cached key/values tuple is at positions\u001b[0m  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 410 \u001b[0m\u001b[2m│   │   \u001b[0mself_attn_past_key_value = past_key_value[:\u001b[94m2\u001b[0m] \u001b[94mif\u001b[0m past_key_value \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[94melse\u001b[0m  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 411 \u001b[2m│   │   \u001b[0mself_attention_outputs = \u001b[96mself\u001b[0m.attention(                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 412 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_states,                                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 413 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask,                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 414 \u001b[0m\u001b[2m│   │   │   \u001b[0mhead_mask,                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/\u001b[0m\u001b[1;33mmodeling_roberta.py\u001b[0m:\u001b[94m338\u001b[0m in   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 335 \u001b[0m\u001b[2m│   │   \u001b[0mpast_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = \u001b[94mNone\u001b[0m,                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 336 \u001b[0m\u001b[2m│   │   \u001b[0moutput_attentions: Optional[\u001b[96mbool\u001b[0m] = \u001b[94mFalse\u001b[0m,                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 337 \u001b[0m\u001b[2m│   \u001b[0m) -> Tuple[torch.Tensor]:                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 338 \u001b[2m│   │   \u001b[0mself_outputs = \u001b[96mself\u001b[0m.self(                                                         \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 339 \u001b[0m\u001b[2m│   │   │   \u001b[0mhidden_states,                                                                \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 340 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask,                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 341 \u001b[0m\u001b[2m│   │   │   \u001b[0mhead_mask,                                                                    \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/models/roberta/\u001b[0m\u001b[1;33mmodeling_roberta.py\u001b[0m:\u001b[94m195\u001b[0m in   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[92mforward\u001b[0m                                                                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 192 \u001b[0m\u001b[2m│   │   \u001b[0mpast_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = \u001b[94mNone\u001b[0m,                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 193 \u001b[0m\u001b[2m│   │   \u001b[0moutput_attentions: Optional[\u001b[96mbool\u001b[0m] = \u001b[94mFalse\u001b[0m,                                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 194 \u001b[0m\u001b[2m│   \u001b[0m) -> Tuple[torch.Tensor]:                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 195 \u001b[2m│   │   \u001b[0mmixed_query_layer = \u001b[96mself\u001b[0m.query(hidden_states)                                     \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 196 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 197 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# If this is instantiated as a cross-attention module, the keys\u001b[0m                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m 198 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# and values come from an encoder; the attention mask needs to be\u001b[0m                 \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/peft/tuners/\u001b[0m\u001b[1;33mlora.py\u001b[0m:\u001b[94m630\u001b[0m in \u001b[92mforward\u001b[0m                       \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m627 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.unmerge()                                                             \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m628 \u001b[0m\u001b[2m│   │   │   \u001b[0mresult = F.linear(x, transpose(\u001b[96mself\u001b[0m.weight, \u001b[96mself\u001b[0m.fan_in_fan_out), bias=\u001b[96mself\u001b[0m.   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m629 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melif\u001b[0m \u001b[96mself\u001b[0m.r[\u001b[96mself\u001b[0m.active_adapter] > \u001b[94m0\u001b[0m \u001b[95mand\u001b[0m \u001b[95mnot\u001b[0m \u001b[96mself\u001b[0m.merged:                          \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m630 \u001b[2m│   │   │   \u001b[0mresult = F.linear(x, transpose(\u001b[96mself\u001b[0m.weight, \u001b[96mself\u001b[0m.fan_in_fan_out), bias=\u001b[96mself\u001b[0m.   \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m631 \u001b[0m\u001b[2m│   │   │   \u001b[0m                                                                               \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m632 \u001b[0m\u001b[2m│   │   │   \u001b[0mx = x.to(\u001b[96mself\u001b[0m.lora_A[\u001b[96mself\u001b[0m.active_adapter].weight.dtype)                        \u001b[31m│\u001b[0m\n",
              "\u001b[31m│\u001b[0m   \u001b[2m633 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
              "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
              "\u001b[1;91mRuntimeError: \u001b[0mCUDA error: CUBLAS_STATUS_NOT_INITIALIZED when calling `\u001b[1;35mcublasCreate\u001b[0m\u001b[1m(\u001b[0mhandle\u001b[1m)\u001b[0m`\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./reward_model_roberta\",\n",
        "    per_device_train_batch_size=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=500,\n",
        "    logging_steps=200,\n",
        "    num_train_epochs=4,\n",
        ")\n",
        "trainer = RewardTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=test,\n",
        "    peft_config=peft_config,\n",
        "    max_length=max_tokens\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "4GDGWZOaqj5m"
      },
      "source": [
        "# 4 OPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ycErfE4qhVT",
        "outputId": "75d84cba-53ea-46ef-9a5d-4de75dfc1b4c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(36140810240, 42481549312)"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.mem_get_info()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "MP2xgpUvqjmY"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OKRhx3vhq8bM"
      },
      "outputs": [],
      "source": [
        "with open('trainin_dataset.json',encoding=\"cp437\", errors='ignore') as json_file:\n",
        "    train = json.load(json_file)\n",
        "\n",
        "max_tokens = 512\n",
        "\n",
        "full = pd.DataFrame(train)\n",
        "\n",
        "full = full[(full['length_0']<=max_tokens)]\n",
        "\n",
        "train, test = train_test_split(full, test_size=0.1, random_state=18625541)\n",
        "\n",
        "train = pd.DataFrame(train)\n",
        "train = train[(train['length_0']<=max_tokens)&(train['length_1']<=max_tokens)]\n",
        "list_of_dicts = train.to_dict(orient='records')\n",
        "train = Dataset.from_list(list_of_dicts)\n",
        "\n",
        "test = pd.DataFrame(test)\n",
        "test = test[(test['length_0']<=max_tokens)&(test['length_1']<=max_tokens)]\n",
        "list_of_dicts = test.to_dict(orient='records')\n",
        "test = Dataset.from_list(list_of_dicts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127,
          "referenced_widgets": [
            "08ca7a06aa284fa389bad740893ecac7",
            "417ffe9183d749b0a45f3eaf03abd072",
            "344048abe5cd4371b996cb304a75d727",
            "bf9fcbecb4fa4627adb40d2e88ded7e5",
            "ba81ec54a983436c85e4413f99992bb0",
            "991ae703c590467e8b4c31c4ee0ce21b",
            "415aed4cfb05436c8b87f450336a3cae",
            "0e65d244b25749d089052ebacb824afc",
            "d115741e57ac4489aa74eb36453907e6",
            "a70d19e17f13478e91f336fc0ab44ae6",
            "8e9f20ffd5694a9cae404fc1711c1caf",
            "2cb829d1656643c698b8310574c48adc",
            "fc6c5972c2f640e39aee0064bff2087d",
            "eb400f1926ef437c98aa6abdead92130",
            "5ffd8db3d48248da88a3a880e34f5953",
            "5b35cd02825b45cbabc82e8627f5080c",
            "7ed5d75b2fc44161b9c1f241db8e8ecc",
            "ff61c11d57964ab29a9b805fe4033546",
            "45b45d6f1c0a40dc9bd4e3dad81ab786",
            "8c224c24c7ab4ce886cda64a43f2e2f3",
            "b13efa5a940d430aaa7f5176acebb5ae",
            "7e66107d38f3447d8b323f4a62ab04c3"
          ]
        },
        "id": "cUf7TZGmKfFB",
        "outputId": "ec3d94ee-7685-488f-b133-f17497e9be78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at AdamG012/chat-opt-350m-reward-deepspeed were not used when initializing OPTForSequenceClassification: ['rwtranrsformer.decoder.layers.11.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.8.fc1.bias', 'rwtranrsformer.decoder.layers.3.fc2.bias', 'rwtranrsformer.decoder.layers.16.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.5.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.12.fc1.bias', 'rwtranrsformer.decoder.layers.15.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.2.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.8.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.0.final_layer_norm.bias', 'rwtranrsformer.decoder.embed_positions.weight', 'rwtranrsformer.decoder.layers.22.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.23.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.14.fc1.weight', 'rwtranrsformer.decoder.layers.10.self_attn.q_proj.weight', 'v_head.weight', 'rwtranrsformer.decoder.layers.13.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.23.fc2.weight', 'rwtranrsformer.decoder.layers.22.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.13.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.10.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.20.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.12.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.10.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.11.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.13.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.16.fc2.weight', 'rwtranrsformer.decoder.layers.7.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.16.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.18.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.22.fc1.weight', 'rwtranrsformer.decoder.layers.7.fc2.bias', 'rwtranrsformer.decoder.layers.19.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.4.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.16.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.2.fc1.weight', 'rwtranrsformer.decoder.layers.12.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.4.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.0.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.18.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.19.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.20.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.4.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.10.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.16.fc1.weight', 'rwtranrsformer.decoder.layers.12.fc2.bias', 'rwtranrsformer.decoder.layers.20.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.21.fc1.bias', 'rwtranrsformer.decoder.layers.8.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.5.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.4.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.2.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.20.fc2.bias', 'rwtranrsformer.decoder.layers.3.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.16.fc2.bias', 'rwtranrsformer.decoder.layers.13.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.7.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.8.fc1.weight', 'rwtranrsformer.decoder.layers.12.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.11.fc1.bias', 'rwtranrsformer.decoder.layers.17.fc2.bias', 'rwtranrsformer.decoder.layers.2.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.23.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.4.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.16.fc1.bias', 'rwtranrsformer.decoder.layers.20.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.2.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.21.fc2.bias', 'rwtranrsformer.decoder.layers.9.fc2.weight', 'rwtranrsformer.decoder.layers.3.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.21.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.9.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.18.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.4.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.10.fc1.weight', 'rwtranrsformer.decoder.layers.14.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.21.fc1.weight', 'rwtranrsformer.decoder.layers.0.fc1.bias', 'rwtranrsformer.decoder.layers.9.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.10.fc2.weight', 'rwtranrsformer.decoder.layers.9.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.22.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.7.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.20.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.6.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.18.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.5.fc2.bias', 'rwtranrsformer.decoder.layers.0.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.11.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.19.fc1.weight', 'rwtranrsformer.decoder.layers.18.fc2.weight', 'rwtranrsformer.decoder.layers.14.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.15.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.18.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.8.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.0.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.2.fc1.bias', 'rwtranrsformer.decoder.layers.9.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.2.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.18.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.20.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.2.fc2.bias', 'rwtranrsformer.decoder.layers.19.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.2.fc2.weight', 'rwtranrsformer.decoder.layers.19.fc1.bias', 'rwtranrsformer.decoder.layers.13.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.6.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.22.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.16.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.19.fc2.bias', 'rwtranrsformer.decoder.layers.22.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.23.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.7.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.20.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.14.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.2.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.18.fc1.bias', 'rwtranrsformer.decoder.layers.2.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.5.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.2.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.14.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.9.fc1.weight', 'rwtranrsformer.decoder.layers.8.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.23.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.13.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.12.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.9.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.0.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.7.fc1.weight', 'rwtranrsformer.decoder.layers.14.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.6.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.1.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.17.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.17.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.6.fc1.bias', 'rwtranrsformer.decoder.layers.8.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.11.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.18.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.17.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.10.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.11.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.10.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.1.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.15.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.1.fc2.bias', 'rwtranrsformer.decoder.layers.3.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.17.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.7.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.9.fc2.bias', 'rwtranrsformer.decoder.layers.23.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.21.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.12.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.15.fc2.bias', 'rwtranrsformer.decoder.layers.6.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.21.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.6.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.21.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.9.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.20.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.16.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.10.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.10.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.15.fc1.bias', 'rwtranrsformer.decoder.layers.14.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.23.fc1.bias', 'rwtranrsformer.decoder.layers.23.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.13.self_attn.q_proj.bias', 'rwtranrsformer.decoder.embed_tokens.weight', 'rwtranrsformer.decoder.layers.13.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.16.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.23.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.14.fc1.bias', 'rwtranrsformer.decoder.layers.3.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.14.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.21.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.17.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.1.self_attn.k_proj.weight', 'rwtranrsformer.decoder.project_out.weight', 'rwtranrsformer.decoder.layers.14.fc2.weight', 'rwtranrsformer.decoder.layers.1.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.17.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.5.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.6.fc2.bias', 'rwtranrsformer.decoder.layers.20.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.10.fc1.bias', 'rwtranrsformer.decoder.layers.19.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.19.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.0.fc2.bias', 'rwtranrsformer.decoder.layers.3.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.12.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.20.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.21.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.15.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.17.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.1.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.16.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.17.fc1.weight', 'rwtranrsformer.decoder.layers.13.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.5.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.4.fc1.bias', 'rwtranrsformer.decoder.layers.0.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.18.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.1.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.14.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.8.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.19.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.4.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.16.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.8.fc2.bias', 'rwtranrsformer.decoder.layers.15.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.22.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.2.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.9.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.5.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.4.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.7.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.18.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.12.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.5.fc2.weight', 'rwtranrsformer.decoder.layers.19.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.21.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.1.fc2.weight', 'rwtranrsformer.decoder.layers.9.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.3.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.5.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.8.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.12.fc2.weight', 'rwtranrsformer.decoder.layers.23.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.21.fc2.weight', 'rwtranrsformer.decoder.layers.6.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.11.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.8.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.5.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.20.fc2.weight', 'rwtranrsformer.decoder.layers.10.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.15.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.15.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.20.fc1.bias', 'rwtranrsformer.decoder.layers.22.fc2.bias', 'rwtranrsformer.decoder.layers.5.fc1.weight', 'rwtranrsformer.decoder.layers.1.fc1.weight', 'rwtranrsformer.decoder.layers.21.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.9.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.21.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.11.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.9.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.0.fc2.weight', 'rwtranrsformer.decoder.layers.0.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.14.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.9.fc1.bias', 'rwtranrsformer.decoder.layers.5.fc1.bias', 'rwtranrsformer.decoder.layers.22.fc1.bias', 'rwtranrsformer.decoder.layers.8.fc2.weight', 'rwtranrsformer.decoder.layers.8.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.6.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.0.fc1.weight', 'rwtranrsformer.decoder.layers.17.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.3.fc2.weight', 'rwtranrsformer.decoder.layers.18.fc1.weight', 'rwtranrsformer.decoder.layers.5.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.3.fc1.bias', 'rwtranrsformer.decoder.layers.6.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.1.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.1.fc1.bias', 'rwtranrsformer.decoder.layers.22.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.2.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.17.fc2.weight', 'rwtranrsformer.decoder.layers.4.fc1.weight', 'rwtranrsformer.decoder.layers.15.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.6.fc1.weight', 'rwtranrsformer.decoder.layers.18.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.11.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.15.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.15.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.11.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.13.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.23.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.17.fc1.bias', 'rwtranrsformer.decoder.layers.4.fc2.bias', 'rwtranrsformer.decoder.layers.14.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.14.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.1.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.3.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.6.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.3.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.21.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.22.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.0.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.19.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.23.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.22.fc2.weight', 'rwtranrsformer.decoder.layers.4.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.7.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.8.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.20.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.16.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.20.fc1.weight', 'rwtranrsformer.decoder.layers.6.fc2.weight', 'rwtranrsformer.decoder.layers.15.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.10.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.22.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.4.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.4.fc2.weight', 'rwtranrsformer.decoder.layers.5.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.17.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.4.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.12.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.11.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.23.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.7.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.15.fc2.weight', 'rwtranrsformer.decoder.layers.12.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.15.fc1.weight', 'rwtranrsformer.decoder.layers.0.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.14.fc2.bias', 'rwtranrsformer.decoder.layers.6.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.23.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.13.self_attn.q_proj.weight', 'rwtranrsformer.decoder.project_in.weight', 'rwtranrsformer.decoder.layers.18.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.12.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.7.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.16.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.7.fc2.weight', 'rwtranrsformer.decoder.layers.11.fc1.weight', 'rwtranrsformer.decoder.layers.16.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.19.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.2.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.5.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.14.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.17.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.10.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.12.fc1.weight', 'rwtranrsformer.decoder.layers.13.fc2.weight', 'rwtranrsformer.decoder.layers.17.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.4.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.12.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.8.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.20.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.3.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.19.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.9.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.11.fc2.bias', 'rwtranrsformer.decoder.layers.16.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.17.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.13.fc1.weight', 'rwtranrsformer.decoder.layers.7.fc1.bias', 'rwtranrsformer.decoder.layers.19.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.22.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.7.self_attn.k_proj.bias', 'rwtranrsformer.decoder.layers.13.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.1.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.0.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.1.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.0.self_attn.k_proj.weight', 'rwtranrsformer.decoder.layers.7.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.7.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.10.fc2.bias', 'rwtranrsformer.decoder.layers.21.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.19.fc2.weight', 'rwtranrsformer.decoder.layers.8.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.5.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.13.fc1.bias', 'rwtranrsformer.decoder.layers.0.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.18.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.22.self_attn.q_proj.weight', 'rwtranrsformer.decoder.layers.15.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.11.self_attn_layer_norm.bias', 'rwtranrsformer.decoder.layers.10.self_attn.out_proj.bias', 'rwtranrsformer.decoder.layers.13.fc2.bias', 'rwtranrsformer.decoder.layers.2.self_attn.out_proj.weight', 'rwtranrsformer.decoder.layers.1.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.22.self_attn_layer_norm.weight', 'rwtranrsformer.decoder.layers.3.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.11.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.9.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.3.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.23.fc1.weight', 'rwtranrsformer.decoder.layers.1.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.3.fc1.weight', 'rwtranrsformer.decoder.layers.18.fc2.bias', 'rwtranrsformer.decoder.layers.6.self_attn.v_proj.weight', 'rwtranrsformer.decoder.layers.11.fc2.weight', 'rwtranrsformer.decoder.layers.12.self_attn.q_proj.bias', 'rwtranrsformer.decoder.layers.19.final_layer_norm.bias', 'rwtranrsformer.decoder.layers.3.self_attn.v_proj.bias', 'rwtranrsformer.decoder.layers.6.final_layer_norm.weight', 'rwtranrsformer.decoder.layers.23.fc2.bias', 'rwtranrsformer.decoder.layers.21.self_attn_layer_norm.bias']\n",
            "- This IS expected if you are initializing OPTForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing OPTForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at AdamG012/chat-opt-350m-reward-deepspeed and are newly initialized: ['decoder.layers.21.self_attn.k_proj.bias', 'decoder.layers.7.self_attn.k_proj.weight', 'decoder.layers.16.self_attn.k_proj.weight', 'decoder.layers.6.fc2.bias', 'decoder.layers.13.final_layer_norm.bias', 'decoder.layers.7.self_attn.v_proj.weight', 'decoder.layers.18.final_layer_norm.weight', 'decoder.layers.8.self_attn.q_proj.weight', 'decoder.layers.4.self_attn.q_proj.bias', 'decoder.layers.11.self_attn.q_proj.weight', 'decoder.layers.19.self_attn.k_proj.weight', 'decoder.layers.18.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.v_proj.weight', 'decoder.layers.20.self_attn.q_proj.weight', 'decoder.layers.7.final_layer_norm.bias', 'decoder.layers.10.self_attn.out_proj.weight', 'decoder.layers.23.final_layer_norm.weight', 'decoder.layers.17.self_attn_layer_norm.weight', 'decoder.layers.1.self_attn.v_proj.bias', 'decoder.layers.7.self_attn.k_proj.bias', 'decoder.layers.1.final_layer_norm.bias', 'decoder.layers.13.fc2.bias', 'decoder.layers.7.self_attn_layer_norm.bias', 'decoder.layers.11.self_attn.v_proj.weight', 'decoder.layers.4.self_attn.out_proj.bias', 'decoder.layers.1.fc2.bias', 'decoder.layers.9.self_attn.q_proj.bias', 'decoder.layers.3.final_layer_norm.weight', 'decoder.layers.22.self_attn_layer_norm.bias', 'decoder.layers.17.self_attn.out_proj.weight', 'decoder.layers.8.final_layer_norm.weight', 'decoder.layers.9.self_attn_layer_norm.bias', 'decoder.layers.10.fc2.weight', 'decoder.layers.11.final_layer_norm.bias', 'decoder.layers.17.fc1.bias', 'decoder.layers.7.self_attn.v_proj.bias', 'decoder.layers.16.self_attn.out_proj.bias', 'decoder.layers.6.self_attn.k_proj.weight', 'decoder.layers.22.self_attn.out_proj.bias', 'decoder.layers.8.fc1.bias', 'decoder.layers.5.fc1.weight', 'decoder.embed_tokens.weight', 'decoder.layers.11.fc1.bias', 'decoder.layers.2.fc1.bias', 'decoder.layers.3.self_attn_layer_norm.weight', 'decoder.layers.12.self_attn.q_proj.bias', 'decoder.layers.8.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.k_proj.bias', 'decoder.layers.15.self_attn.q_proj.weight', 'decoder.layers.7.fc1.weight', 'decoder.layers.4.fc1.weight', 'decoder.layers.1.self_attn.k_proj.weight', 'decoder.layers.13.final_layer_norm.weight', 'decoder.layers.14.self_attn.k_proj.weight', 'decoder.layers.9.fc1.bias', 'decoder.layers.17.self_attn.q_proj.bias', 'decoder.layers.18.self_attn_layer_norm.weight', 'decoder.layers.8.self_attn.out_proj.weight', 'decoder.layers.4.final_layer_norm.weight', 'decoder.layers.15.self_attn.v_proj.bias', 'decoder.layers.23.self_attn.k_proj.weight', 'decoder.layers.19.self_attn.q_proj.weight', 'decoder.layers.6.self_attn.k_proj.bias', 'decoder.layers.10.self_attn.q_proj.bias', 'decoder.layers.23.self_attn.v_proj.weight', 'decoder.layers.21.fc1.weight', 'decoder.layers.14.self_attn.v_proj.bias', 'decoder.layers.6.fc1.bias', 'decoder.layers.13.self_attn_layer_norm.bias', 'decoder.layers.22.self_attn.out_proj.weight', 'decoder.layers.19.final_layer_norm.bias', 'decoder.layers.5.fc2.weight', 'decoder.layers.18.fc2.weight', 'decoder.layers.19.fc2.bias', 'decoder.layers.0.fc1.bias', 'decoder.layers.22.fc1.bias', 'decoder.layers.5.self_attn.v_proj.bias', 'decoder.layers.12.self_attn_layer_norm.bias', 'decoder.layers.21.fc2.weight', 'decoder.layers.23.self_attn.out_proj.weight', 'decoder.layers.6.self_attn.q_proj.weight', 'decoder.layers.6.fc2.weight', 'decoder.layers.5.self_attn_layer_norm.bias', 'decoder.layers.20.self_attn_layer_norm.weight', 'decoder.layers.3.self_attn.out_proj.weight', 'decoder.layers.4.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn.k_proj.weight', 'decoder.layers.4.self_attn.q_proj.weight', 'decoder.layers.2.fc2.weight', 'decoder.layers.20.final_layer_norm.bias', 'decoder.layers.15.fc1.bias', 'decoder.layers.13.self_attn.k_proj.bias', 'decoder.layers.2.self_attn.q_proj.bias', 'decoder.layers.17.self_attn.v_proj.bias', 'decoder.layers.19.self_attn_layer_norm.weight', 'decoder.layers.8.self_attn_layer_norm.bias', 'decoder.layers.3.self_attn.v_proj.weight', 'decoder.layers.11.self_attn.v_proj.bias', 'decoder.layers.18.self_attn.v_proj.weight', 'decoder.layers.22.fc1.weight', 'decoder.layers.22.final_layer_norm.weight', 'decoder.layers.3.fc1.weight', 'decoder.layers.20.fc2.weight', 'decoder.layers.0.final_layer_norm.weight', 'decoder.layers.5.fc2.bias', 'decoder.layers.6.self_attn_layer_norm.bias', 'decoder.layers.15.self_attn_layer_norm.weight', 'decoder.layers.8.self_attn.k_proj.bias', 'decoder.layers.13.self_attn.q_proj.weight', 'decoder.layers.18.self_attn.q_proj.weight', 'decoder.layers.3.self_attn_layer_norm.bias', 'decoder.layers.23.self_attn.k_proj.bias', 'decoder.layers.15.self_attn.q_proj.bias', 'decoder.layers.9.final_layer_norm.bias', 'decoder.layers.6.self_attn.out_proj.bias', 'decoder.layers.1.final_layer_norm.weight', 'decoder.layers.8.self_attn.v_proj.weight', 'decoder.project_in.weight', 'decoder.layers.18.fc2.bias', 'decoder.layers.17.fc1.weight', 'decoder.layers.8.fc2.weight', 'decoder.layers.20.self_attn.k_proj.bias', 'decoder.layers.21.self_attn_layer_norm.bias', 'decoder.layers.7.self_attn_layer_norm.weight', 'decoder.layers.11.self_attn.q_proj.bias', 'decoder.layers.20.fc2.bias', 'decoder.layers.14.self_attn.q_proj.bias', 'decoder.layers.20.self_attn.v_proj.weight', 'decoder.layers.6.final_layer_norm.bias', 'decoder.layers.2.self_attn.v_proj.bias', 'decoder.layers.13.self_attn_layer_norm.weight', 'decoder.layers.2.self_attn.q_proj.weight', 'decoder.layers.22.self_attn.k_proj.weight', 'decoder.layers.13.self_attn.v_proj.weight', 'decoder.layers.23.self_attn_layer_norm.weight', 'decoder.layers.6.fc1.weight', 'decoder.layers.3.fc1.bias', 'decoder.layers.1.fc1.weight', 'decoder.layers.8.self_attn.out_proj.bias', 'decoder.layers.10.self_attn.q_proj.weight', 'decoder.layers.10.self_attn.k_proj.bias', 'decoder.layers.20.fc1.weight', 'decoder.layers.21.self_attn_layer_norm.weight', 'decoder.layers.23.self_attn.q_proj.weight', 'decoder.layers.17.final_layer_norm.bias', 'decoder.layers.4.self_attn.k_proj.bias', 'decoder.layers.23.fc1.weight', 'decoder.layers.11.self_attn.k_proj.bias', 'decoder.layers.5.final_layer_norm.weight', 'decoder.layers.9.self_attn_layer_norm.weight', 'decoder.layers.23.self_attn.out_proj.bias', 'decoder.layers.2.fc2.bias', 'decoder.layers.9.self_attn.k_proj.weight', 'decoder.layers.8.fc2.bias', 'decoder.layers.3.self_attn.v_proj.bias', 'decoder.layers.5.self_attn.out_proj.bias', 'decoder.layers.17.final_layer_norm.weight', 'decoder.layers.11.self_attn.out_proj.bias', 'decoder.layers.6.self_attn.q_proj.bias', 'decoder.layers.11.self_attn_layer_norm.weight', 'decoder.layers.13.self_attn.v_proj.bias', 'decoder.layers.19.self_attn.q_proj.bias', 'decoder.layers.15.final_layer_norm.bias', 'decoder.layers.14.self_attn.k_proj.bias', 'decoder.layers.15.self_attn.v_proj.weight', 'decoder.layers.13.self_attn.out_proj.weight', 'decoder.layers.4.self_attn.k_proj.weight', 'decoder.layers.22.self_attn.q_proj.bias', 'decoder.layers.19.fc2.weight', 'decoder.layers.22.fc2.weight', 'decoder.layers.16.self_attn.k_proj.bias', 'decoder.layers.15.final_layer_norm.weight', 'decoder.layers.16.self_attn.v_proj.weight', 'decoder.layers.5.self_attn_layer_norm.weight', 'decoder.layers.17.self_attn.out_proj.bias', 'decoder.layers.2.self_attn.out_proj.bias', 'decoder.layers.9.self_attn.v_proj.weight', 'decoder.layers.19.self_attn.out_proj.weight', 'decoder.layers.15.fc2.bias', 'decoder.layers.14.self_attn.out_proj.weight', 'decoder.project_out.weight', 'decoder.layers.10.final_layer_norm.bias', 'decoder.layers.11.self_attn.k_proj.weight', 'decoder.layers.22.self_attn.v_proj.weight', 'decoder.layers.2.self_attn.out_proj.weight', 'decoder.layers.0.self_attn.k_proj.bias', 'decoder.layers.14.fc1.bias', 'decoder.layers.20.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.v_proj.weight', 'decoder.layers.10.self_attn.v_proj.weight', 'decoder.layers.19.self_attn.v_proj.bias', 'decoder.layers.4.fc2.bias', 'decoder.layers.23.self_attn.v_proj.bias', 'decoder.layers.1.self_attn.k_proj.bias', 'decoder.layers.6.self_attn.v_proj.bias', 'decoder.layers.16.final_layer_norm.weight', 'decoder.layers.3.final_layer_norm.bias', 'decoder.layers.10.fc1.weight', 'decoder.layers.7.fc2.bias', 'decoder.layers.7.self_attn.out_proj.weight', 'decoder.layers.8.self_attn.k_proj.weight', 'decoder.layers.22.fc2.bias', 'decoder.layers.9.fc2.bias', 'decoder.layers.16.fc1.weight', 'decoder.layers.6.final_layer_norm.weight', 'decoder.layers.9.final_layer_norm.weight', 'decoder.layers.11.self_attn_layer_norm.bias', 'decoder.layers.7.fc1.bias', 'decoder.layers.0.self_attn_layer_norm.weight', 'decoder.layers.19.final_layer_norm.weight', 'decoder.layers.14.fc1.weight', 'decoder.layers.15.self_attn.k_proj.bias', 'decoder.layers.0.fc2.bias', 'decoder.layers.10.self_attn_layer_norm.weight', 'decoder.layers.12.self_attn.v_proj.bias', 'decoder.layers.2.self_attn.k_proj.bias', 'decoder.layers.5.fc1.bias', 'decoder.layers.21.self_attn.q_proj.weight', 'decoder.layers.12.self_attn.out_proj.bias', 'decoder.layers.14.final_layer_norm.weight', 'decoder.layers.3.fc2.bias', 'decoder.layers.6.self_attn.v_proj.weight', 'decoder.layers.12.self_attn.v_proj.weight', 'decoder.layers.16.self_attn_layer_norm.bias', 'decoder.layers.17.self_attn.v_proj.weight', 'decoder.layers.3.self_attn.k_proj.bias', 'decoder.layers.18.self_attn.v_proj.bias', 'decoder.layers.20.final_layer_norm.weight', 'decoder.layers.12.self_attn.k_proj.bias', 'decoder.layers.16.fc2.weight', 'decoder.layers.3.self_attn.out_proj.bias', 'decoder.layers.2.final_layer_norm.bias', 'decoder.layers.1.self_attn_layer_norm.weight', 'decoder.layers.19.fc1.bias', 'decoder.layers.0.fc2.weight', 'decoder.layers.1.self_attn.v_proj.weight', 'decoder.layers.19.fc1.weight', 'decoder.layers.22.self_attn.v_proj.bias', 'decoder.layers.12.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.k_proj.weight', 'decoder.layers.1.self_attn.out_proj.weight', 'decoder.layers.1.fc2.weight', 'decoder.layers.18.fc1.bias', 'decoder.layers.12.fc1.bias', 'decoder.layers.11.fc2.weight', 'decoder.layers.13.self_attn.q_proj.bias', 'decoder.layers.9.self_attn.out_proj.weight', 'decoder.layers.0.self_attn_layer_norm.bias', 'decoder.embed_positions.weight', 'decoder.layers.1.self_attn_layer_norm.bias', 'decoder.layers.15.self_attn_layer_norm.bias', 'decoder.layers.2.fc1.weight', 'decoder.layers.13.self_attn.k_proj.weight', 'decoder.layers.18.self_attn.k_proj.weight', 'decoder.layers.8.final_layer_norm.bias', 'decoder.layers.20.self_attn_layer_norm.bias', 'decoder.layers.14.self_attn.q_proj.weight', 'decoder.layers.18.self_attn.q_proj.bias', 'decoder.layers.12.fc2.bias', 'decoder.layers.17.self_attn.q_proj.weight', 'decoder.layers.16.final_layer_norm.bias', 'decoder.layers.17.fc2.weight', 'decoder.layers.22.self_attn_layer_norm.weight', 'decoder.layers.5.self_attn.out_proj.weight', 'decoder.layers.14.self_attn.v_proj.weight', 'decoder.layers.20.self_attn.q_proj.bias', 'decoder.layers.0.self_attn.out_proj.weight', 'decoder.layers.15.fc1.weight', 'decoder.layers.1.self_attn.out_proj.bias', 'decoder.layers.1.self_attn.q_proj.weight', 'decoder.layers.13.fc1.weight', 'decoder.layers.20.self_attn.out_proj.weight', 'decoder.layers.19.self_attn.k_proj.bias', 'decoder.layers.5.final_layer_norm.bias', 'decoder.layers.16.fc1.bias', 'decoder.layers.2.self_attn_layer_norm.weight', 'decoder.layers.17.self_attn.k_proj.bias', 'decoder.layers.9.self_attn.q_proj.weight', 'decoder.layers.18.self_attn.k_proj.bias', 'decoder.layers.10.self_attn.out_proj.bias', 'decoder.layers.20.self_attn.out_proj.bias', 'decoder.layers.12.fc2.weight', 'decoder.layers.22.self_attn.q_proj.weight', 'decoder.layers.4.fc2.weight', 'decoder.layers.15.self_attn.k_proj.weight', 'decoder.layers.10.fc1.bias', 'decoder.layers.14.self_attn_layer_norm.bias', 'decoder.layers.18.final_layer_norm.bias', 'decoder.layers.5.self_attn.q_proj.weight', 'decoder.layers.3.self_attn.q_proj.bias', 'decoder.layers.15.self_attn.out_proj.weight', 'decoder.layers.16.self_attn_layer_norm.weight', 'decoder.layers.23.final_layer_norm.bias', 'decoder.layers.16.self_attn.out_proj.weight', 'decoder.layers.10.self_attn.k_proj.weight', 'decoder.layers.22.self_attn.k_proj.bias', 'decoder.layers.23.self_attn.q_proj.bias', 'score.weight', 'decoder.layers.14.fc2.weight', 'decoder.layers.4.self_attn.v_proj.bias', 'decoder.layers.11.self_attn.out_proj.weight', 'decoder.layers.1.fc1.bias', 'decoder.layers.14.self_attn_layer_norm.weight', 'decoder.layers.10.self_attn.v_proj.bias', 'decoder.layers.13.fc1.bias', 'decoder.layers.3.self_attn.k_proj.weight', 'decoder.layers.2.final_layer_norm.weight', 'decoder.layers.9.fc1.weight', 'decoder.layers.16.fc2.bias', 'decoder.layers.4.self_attn.out_proj.weight', 'decoder.layers.3.self_attn.q_proj.weight', 'decoder.layers.18.fc1.weight', 'decoder.layers.6.self_attn_layer_norm.weight', 'decoder.layers.23.self_attn_layer_norm.bias', 'decoder.layers.10.fc2.bias', 'decoder.layers.23.fc2.bias', 'decoder.layers.7.fc2.weight', 'decoder.layers.13.fc2.weight', 'decoder.layers.8.self_attn_layer_norm.weight', 'decoder.layers.18.self_attn.out_proj.bias', 'decoder.layers.8.self_attn.q_proj.bias', 'decoder.layers.9.fc2.weight', 'decoder.layers.21.self_attn.out_proj.bias', 'decoder.layers.12.final_layer_norm.bias', 'decoder.layers.18.self_attn_layer_norm.bias', 'decoder.layers.0.self_attn.v_proj.bias', 'decoder.layers.8.fc1.weight', 'decoder.layers.12.self_attn.out_proj.weight', 'decoder.layers.16.self_attn.q_proj.bias', 'decoder.layers.21.self_attn.out_proj.weight', 'decoder.layers.19.self_attn_layer_norm.bias', 'decoder.layers.4.self_attn_layer_norm.bias', 'decoder.layers.10.self_attn_layer_norm.bias', 'decoder.layers.17.fc2.bias', 'decoder.layers.19.self_attn.out_proj.bias', 'decoder.layers.21.final_layer_norm.weight', 'decoder.layers.2.self_attn.k_proj.weight', 'decoder.layers.6.self_attn.out_proj.weight', 'decoder.layers.2.self_attn_layer_norm.bias', 'decoder.layers.10.final_layer_norm.weight', 'decoder.layers.12.self_attn.k_proj.weight', 'decoder.layers.14.fc2.bias', 'decoder.layers.21.fc1.bias', 'decoder.layers.1.self_attn.q_proj.bias', 'decoder.layers.12.final_layer_norm.weight', 'decoder.layers.23.fc2.weight', 'decoder.layers.20.self_attn.v_proj.bias', 'decoder.layers.12.fc1.weight', 'decoder.layers.2.self_attn.v_proj.weight', 'decoder.layers.0.fc1.weight', 'decoder.layers.16.self_attn.v_proj.bias', 'decoder.layers.17.self_attn_layer_norm.bias', 'decoder.layers.14.self_attn.out_proj.bias', 'decoder.layers.9.self_attn.k_proj.bias', 'decoder.layers.9.self_attn.out_proj.bias', 'decoder.layers.17.self_attn.k_proj.weight', 'decoder.layers.11.fc1.weight', 'decoder.layers.23.fc1.bias', 'decoder.layers.11.fc2.bias', 'decoder.layers.7.self_attn.out_proj.bias', 'decoder.layers.21.self_attn.q_proj.bias', 'decoder.layers.12.self_attn_layer_norm.weight', 'decoder.layers.21.final_layer_norm.bias', 'decoder.layers.0.final_layer_norm.bias', 'decoder.layers.0.self_attn.q_proj.weight', 'decoder.layers.7.self_attn.q_proj.bias', 'decoder.layers.21.fc2.bias', 'decoder.layers.21.self_attn.v_proj.bias', 'decoder.layers.11.final_layer_norm.weight', 'decoder.layers.7.self_attn.q_proj.weight', 'decoder.layers.4.fc1.bias', 'decoder.layers.22.final_layer_norm.bias', 'decoder.layers.5.self_attn.v_proj.weight', 'decoder.layers.20.fc1.bias', 'decoder.layers.4.final_layer_norm.bias', 'decoder.layers.7.final_layer_norm.weight', 'decoder.layers.15.fc2.weight', 'decoder.layers.15.self_attn.out_proj.bias', 'decoder.layers.21.self_attn.v_proj.weight', 'decoder.layers.19.self_attn.v_proj.weight', 'decoder.layers.9.self_attn.v_proj.bias', 'decoder.layers.21.self_attn.k_proj.weight', 'decoder.layers.0.self_attn.q_proj.bias', 'decoder.layers.3.fc2.weight', 'decoder.layers.5.self_attn.q_proj.bias', 'decoder.layers.14.final_layer_norm.bias', 'decoder.layers.13.self_attn.out_proj.bias', 'decoder.layers.16.self_attn.q_proj.weight', 'decoder.layers.0.self_attn.out_proj.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08ca7a06aa284fa389bad740893ecac7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/106 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2cb829d1656643c698b8310574c48adc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/469 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = \"AdamG012/chat-opt-350m-reward-deepspeed\"\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.config.pad_token_id = model.config.eos_token_id\n",
        "\n",
        "def formatting_func(examples):\n",
        "    kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": max_tokens, \"return_tensors\": \"pt\"}\n",
        "\n",
        "    # Prepend the prompt and a line break to the original_response and response-1 fields.\n",
        "    prompt_plus_chosen_response = examples[\"chat_1\"]\n",
        "    prompt_plus_rejected_response = examples[\"chat_0\"]\n",
        "\n",
        "    # Then tokenize these modified fields.\n",
        "    tokens_chosen = tokenizer.encode_plus(prompt_plus_chosen_response, **kwargs)\n",
        "    tokens_rejected = tokenizer.encode_plus(prompt_plus_rejected_response, **kwargs)\n",
        "\n",
        "    return {\n",
        "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0], \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
        "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0], \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n",
        "    }\n",
        "\n",
        "test = test.map(formatting_func)\n",
        "train = train.map(formatting_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "h5v89BJpsf2C",
        "outputId": "7f1f455c-7bd1-45bc-d60b-3a81aa4cbff5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8a7b1159285a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m training_args = TrainingArguments(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./reward_model_opt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mper_device_train_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mevaluation_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"steps\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'TrainingArguments' is not defined"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./reward_model_opt\",\n",
        "    per_device_train_batch_size=1,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=1e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=10,\n",
        "    max_steps=1,\n",
        "    logging_steps=1,\n",
        ")\n",
        "trainer = RewardTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=test,\n",
        "    max_length=max_tokens,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "BPrGhhVuqqVg"
      },
      "source": [
        "# 5 DEBERTA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHzmiNLtr661",
        "outputId": "106fe2c0-5b17-4225-d87e-b51b75828c93"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(22740008960, 42481549312)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "torch.cuda.mem_get_info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "se4zHra9r5fo"
      },
      "outputs": [],
      "source": [
        "with open('training_dataset.json',encoding=\"cp437\", errors='ignore') as json_file:\n",
        "    train = json.load(json_file)\n",
        "\n",
        "max_tokens = 512\n",
        "\n",
        "full = pd.DataFrame(train)\n",
        "\n",
        "full = full[(full['length_0']<=max_tokens)]\n",
        "\n",
        "train, test = train_test_split(full, test_size=0.1, random_state=18625541)\n",
        "\n",
        "train = pd.DataFrame(train)\n",
        "train = train[(train['length_0']<=max_tokens)&(train['length_1']<=max_tokens)]\n",
        "list_of_dicts = train.to_dict(orient='records')\n",
        "train = Dataset.from_list(list_of_dicts)\n",
        "\n",
        "test = pd.DataFrame(test)\n",
        "test = test[(test['length_0']<=max_tokens)&(test['length_1']<=max_tokens)]\n",
        "list_of_dicts = test.to_dict(orient='records')\n",
        "test = Dataset.from_list(list_of_dicts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qPjgAYdt-xl",
        "outputId": "39457db3-de1d-4bcb-c93a-84660cfeb718"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2716, 303)"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train), len(test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "e8e873fab9cd4ce582d7d6e900aadc25",
            "ede80a63d92c4b15a9257204ba27868b",
            "8496c7c2340c4ababfc5d5002509d2f1",
            "7ade8ac56cd84b51ad560598ff2fd547",
            "4405344bef97482292a74304f571f0df",
            "41706078fbbb415284d518cd0abdeb96",
            "4bc2997502ad46fdbd6b633f5867a508",
            "c0c0c1d5f8934274abd15684fac4a1b2",
            "0247154c67404ca9a84944535b1f4a7d",
            "84f8ca37318e4846a38a9fd86567fd49",
            "94efdd3ef8e5465ca716ec316c008c7c",
            "f65da7ad38af40279e1510d46e70c33f",
            "1fddf94e4f974252ace1480e436b61a6",
            "96773e984bd54f6090923d6dc0b6f9cf",
            "f7076ff5bdc64ac8833d8abf73131156",
            "b48dc83f2a0c4211bd696f09219550ca",
            "ff418fc49cdc46bfbe28eaf149adbb5b",
            "c7c55ef5649849c1a9ef58e77845d9a9",
            "462fdfe5256b493e8adaa6ecb55456ea",
            "f3bb89eecc5b4cb0b12fa4a6d566b40f",
            "84837790f4e8464db6ac2819a2173b1c",
            "4d872210910a41429be598357ac15801"
          ]
        },
        "id": "eTXiePdwNf95",
        "outputId": "fdb86477-29da-4fa0-b9a6-6c0e6d1e9e1b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e8e873fab9cd4ce582d7d6e900aadc25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/303 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f65da7ad38af40279e1510d46e70c33f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2716 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = \"OpenAssistant/reward-model-deberta-v3-large-v2\"\n",
        "model_nmae = \"reward_model_debert_augmented/checkpoint-500\"\n",
        "\n",
        "\n",
        "model1 = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    model1.config.pad_token_id = model1.config.eos_token_id\n",
        "\n",
        "def formatting_func(examples):\n",
        "    kwargs = {\"padding\": \"max_length\", \"truncation\": True, \"max_length\": max_tokens, \"return_tensors\": \"pt\"}\n",
        "\n",
        "    # Prepend the prompt and a line break to the original_response and response-1 fields.\n",
        "    prompt_plus_chosen_response = examples[\"chat_1\"]\n",
        "    prompt_plus_rejected_response = examples[\"chat_0\"]\n",
        "\n",
        "    # Then tokenize these modified fields.\n",
        "    tokens_chosen = tokenizer.encode_plus(prompt_plus_chosen_response, **kwargs)\n",
        "    tokens_rejected = tokenizer.encode_plus(prompt_plus_rejected_response, **kwargs)\n",
        "\n",
        "    return {\n",
        "        \"input_ids_chosen\": tokens_chosen[\"input_ids\"][0], \"attention_mask_chosen\": tokens_chosen[\"attention_mask\"][0],\n",
        "        \"input_ids_rejected\": tokens_rejected[\"input_ids\"][0], \"attention_mask_rejected\": tokens_rejected[\"attention_mask\"][0]\n",
        "    }\n",
        "\n",
        "test = test.map(formatting_func)\n",
        "train = train.map(formatting_func)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "id": "hDF1Zd63uTeB",
        "outputId": "a2c2f280-8065-4d8d-89b2-bfe1e1c210f9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/reward_trainer.py:136: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1358' max='1358' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1358/1358 23:00, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.538800</td>\n",
              "      <td>0.481489</td>\n",
              "      <td>0.749175</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.481800</td>\n",
              "      <td>0.449655</td>\n",
              "      <td>0.792079</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.481100</td>\n",
              "      <td>0.409695</td>\n",
              "      <td>0.815182</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.415800</td>\n",
              "      <td>0.397066</td>\n",
              "      <td>0.825083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.375400</td>\n",
              "      <td>0.382414</td>\n",
              "      <td>0.834983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.376500</td>\n",
              "      <td>0.382072</td>\n",
              "      <td>0.841584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.397000</td>\n",
              "      <td>0.373335</td>\n",
              "      <td>0.851485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.349400</td>\n",
              "      <td>0.364485</td>\n",
              "      <td>0.848185</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.283100</td>\n",
              "      <td>0.368568</td>\n",
              "      <td>0.851485</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2395: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1358, training_loss=0.4096897020606826, metrics={'train_runtime': 1382.3841, 'train_samples_per_second': 3.929, 'train_steps_per_second': 0.982, 'total_flos': 0.0, 'train_loss': 0.4096897020606826, 'epoch': 2.0})"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./reward_model_debert_augmented\",\n",
        "    per_device_train_batch_size=4,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    learning_rate=2e-6,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=300,\n",
        "    logging_steps=150,\n",
        "    num_train_epochs=2,\n",
        ")\n",
        "trainer = RewardTrainer(\n",
        "    model=model1,\n",
        "    args=training_args,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train,\n",
        "    eval_dataset=test,\n",
        "    max_length=max_tokens\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OyaTzJag5TOF"
      },
      "outputs": [],
      "source": [
        "with open('eval_dataset.json',encoding=\"cp437\", errors='ignore') as json_file:\n",
        "    eval = json.load(json_file)\n",
        "\n",
        "max_tokens = 512\n",
        "\n",
        "eval = pd.DataFrame(eval)\n",
        "list_of_dicts = eval.to_dict(orient='records')\n",
        "eval = Dataset.from_list(list_of_dicts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "3829aa1a0733462ba46c978396577726",
            "0212cce156a64da08bc4438ad4f2f9da",
            "f03fd0cccab645588c8214ab5ca34690",
            "ed96af72f21141a7ae2f3cfdc8999035",
            "ad0565a3e4be4031a76dcfbeb33ed330",
            "daddc964e23e4674bc9dea9a7d9ff915",
            "aa0c77329b284259a66270ddaf09b4d8",
            "e0203a495e1b4154b7c6f9d63087ad0c",
            "42742af3f32643a4a3c5199d9accc3a9",
            "87fd461cf9104848bd58332cceb09524",
            "a058da3d27e142c386397d944aff33ba"
          ]
        },
        "id": "zYJMPG6B5Vx4",
        "outputId": "04686598-9ca0-4d57-d9b4-dde7535aa012"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3829aa1a0733462ba46c978396577726",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/13586 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "\n",
        "eval_encoded = eval.map(lambda examples: tokenizer(examples['chat'], truncation=True, padding='longest',max_length=max_tokens), batched=True)\n",
        "\n",
        "\n",
        "eval_encoded.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "\n",
        "dataloader = DataLoader(eval_encoded, batch_size=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHUCvu465X6y",
        "outputId": "2d30449e-9755-4fd0-b364-d500afae04eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3397/3397 [07:49<00:00,  7.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "13586\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Disable gradient calculation\n",
        "outputs = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(dataloader):\n",
        "        # Move batch to device\n",
        "        batch = {k: v.to('cuda') for k, v in batch.items()}\n",
        "        # Get model's output\n",
        "        output = model1(**batch)\n",
        "        outputs.append(output)\n",
        "# Flatten the list of batched outputs\n",
        "flattened_outputs = np.concatenate([batch.logits.cpu().numpy() for batch in outputs], axis=0)\n",
        "print(len(flattened_outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kvVa5NtW5d5t"
      },
      "outputs": [],
      "source": [
        "eval_df = pd.DataFrame(list_of_dicts)\n",
        "eval_df['predictions'] = flattened_outputs\n",
        "eval_df.to_json(\"results_deberta.json\", orient=\"records\", indent=4)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0212cce156a64da08bc4438ad4f2f9da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_daddc964e23e4674bc9dea9a7d9ff915",
            "placeholder": "​",
            "style": "IPY_MODEL_aa0c77329b284259a66270ddaf09b4d8",
            "value": "Map: 100%"
          }
        },
        "0247154c67404ca9a84944535b1f4a7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0402d6de68554f109dc3d65e19c04b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05b4f0c8c4694c78af264eeba1c9a9d2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08ca7a06aa284fa389bad740893ecac7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_417ffe9183d749b0a45f3eaf03abd072",
              "IPY_MODEL_344048abe5cd4371b996cb304a75d727",
              "IPY_MODEL_bf9fcbecb4fa4627adb40d2e88ded7e5"
            ],
            "layout": "IPY_MODEL_ba81ec54a983436c85e4413f99992bb0"
          }
        },
        "0d9b0a83816a4fa6b0c1836aae949154": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5934c316e22a467995ace2070ff8c111",
            "placeholder": "​",
            "style": "IPY_MODEL_f00ee97110ef4dd19c87d725864208dc",
            "value": "Map: 100%"
          }
        },
        "0e65d244b25749d089052ebacb824afc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15bb07895cb4418dbdb5d78de92fb345": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d0be9465d834418b0c092e68cb17e29",
            "placeholder": "​",
            "style": "IPY_MODEL_72fc7945121745699e04d508a3649cd8",
            "value": " 1952/1970 [00:05&lt;00:00, 434.05 examples/s]"
          }
        },
        "15e3d6222ff84981b8c903f8f7a32e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a920a25f6a04a15abee42958e5e0993": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ded6961e5634cd3b845f7f916dd299f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fddf94e4f974252ace1480e436b61a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff418fc49cdc46bfbe28eaf149adbb5b",
            "placeholder": "​",
            "style": "IPY_MODEL_c7c55ef5649849c1a9ef58e77845d9a9",
            "value": "Map:  99%"
          }
        },
        "20c774598fb848868d7064367dd4742c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e68cd4a5423d44429c1ebaaf6b872c9d",
            "max": 1970,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d3a39d9de0cd4233865443fda32e8ad8",
            "value": 1970
          }
        },
        "21c7d64071f34c83a0c5f025dbcc52cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2232a4410850476abf0091ff2a9fb5ac": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "27b84f164d224576b39569b8e173efd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05b4f0c8c4694c78af264eeba1c9a9d2",
            "placeholder": "​",
            "style": "IPY_MODEL_8e658aa0ccb44bf6979266f0c8410847",
            "value": "Map:  99%"
          }
        },
        "28eb21e19aae489aaaf9de952c551f21": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2cb829d1656643c698b8310574c48adc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc6c5972c2f640e39aee0064bff2087d",
              "IPY_MODEL_eb400f1926ef437c98aa6abdead92130",
              "IPY_MODEL_5ffd8db3d48248da88a3a880e34f5953"
            ],
            "layout": "IPY_MODEL_5b35cd02825b45cbabc82e8627f5080c"
          }
        },
        "2db6c5e732764aa88fca6a626c92e779": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31d7cff0b77a407c8b91edbcfb0ab7f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28eb21e19aae489aaaf9de952c551f21",
            "placeholder": "​",
            "style": "IPY_MODEL_c87ad2ecbd5e48bdb515c52f9a53c885",
            "value": " 494/498 [00:01&lt;00:00, 426.98 examples/s]"
          }
        },
        "344048abe5cd4371b996cb304a75d727": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e65d244b25749d089052ebacb824afc",
            "max": 106,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d115741e57ac4489aa74eb36453907e6",
            "value": 106
          }
        },
        "362ab3fa194b4d1e8b96c37994a6b2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a70c0d52fb0a4489b55bac7aabaa46cb",
            "placeholder": "​",
            "style": "IPY_MODEL_d543dba028c6436db9b2dd179b05a10f",
            "value": "Map:  93%"
          }
        },
        "37cc4039832248b18962523ebd67fb8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d44a288f184641f08a6b9b788e92f85b",
              "IPY_MODEL_20c774598fb848868d7064367dd4742c",
              "IPY_MODEL_15bb07895cb4418dbdb5d78de92fb345"
            ],
            "layout": "IPY_MODEL_45ef6ba576984912ac8d650e72944cbd"
          }
        },
        "3829aa1a0733462ba46c978396577726": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0212cce156a64da08bc4438ad4f2f9da",
              "IPY_MODEL_f03fd0cccab645588c8214ab5ca34690",
              "IPY_MODEL_ed96af72f21141a7ae2f3cfdc8999035"
            ],
            "layout": "IPY_MODEL_ad0565a3e4be4031a76dcfbeb33ed330"
          }
        },
        "3cd7427d9942487880d36812919c6498": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85fdf22d367847f0b2217f4c17f1ecae",
            "max": 2716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cb90234c1d44482a7542b030a630093",
            "value": 2716
          }
        },
        "415aed4cfb05436c8b87f450336a3cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41706078fbbb415284d518cd0abdeb96": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "417ffe9183d749b0a45f3eaf03abd072": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_991ae703c590467e8b4c31c4ee0ce21b",
            "placeholder": "​",
            "style": "IPY_MODEL_415aed4cfb05436c8b87f450336a3cae",
            "value": "Map:  61%"
          }
        },
        "42742af3f32643a4a3c5199d9accc3a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4405344bef97482292a74304f571f0df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "45b45d6f1c0a40dc9bd4e3dad81ab786": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45ef6ba576984912ac8d650e72944cbd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "462fdfe5256b493e8adaa6ecb55456ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "479dfdbe1ff746aca5bf9c91d322a7d4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bc2997502ad46fdbd6b633f5867a508": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4cb341a7a3934f2ebda6ac8232d02451": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27b84f164d224576b39569b8e173efd4",
              "IPY_MODEL_5d89d8982b434ef58bd3d924c1ceeb96",
              "IPY_MODEL_838b2d52e34d47c6beaf672b03a47d81"
            ],
            "layout": "IPY_MODEL_2232a4410850476abf0091ff2a9fb5ac"
          }
        },
        "4cb90234c1d44482a7542b030a630093": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d872210910a41429be598357ac15801": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e66fbc2b4214a8698ccbd9a0ae4319b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "551a4623c15c403bb36ad0a9ba131535": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5934c316e22a467995ace2070ff8c111": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b35cd02825b45cbabc82e8627f5080c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "5d0be9465d834418b0c092e68cb17e29": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d89d8982b434ef58bd3d924c1ceeb96": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8633274057b3478e93904f761eafa08d",
            "max": 1970,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5efd8fa19c8c4bd18190c831d15681e3",
            "value": 1970
          }
        },
        "5efd8fa19c8c4bd18190c831d15681e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5ffd8db3d48248da88a3a880e34f5953": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b13efa5a940d430aaa7f5176acebb5ae",
            "placeholder": "​",
            "style": "IPY_MODEL_7e66107d38f3447d8b323f4a62ab04c3",
            "value": " 465/469 [00:00&lt;00:00, 708.62 examples/s]"
          }
        },
        "61c53c66b1e84922982125745e847904": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b146db5409e44b29a14da22b2a036a54",
            "max": 498,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6910f14f801e40a0bb9388128a6dc059",
            "value": 498
          }
        },
        "68dc60728f114f63a2947544f30ef6b5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6910f14f801e40a0bb9388128a6dc059": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "72fc7945121745699e04d508a3649cd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ade8ac56cd84b51ad560598ff2fd547": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84f8ca37318e4846a38a9fd86567fd49",
            "placeholder": "​",
            "style": "IPY_MODEL_94efdd3ef8e5465ca716ec316c008c7c",
            "value": " 301/303 [00:00&lt;00:00, 363.07 examples/s]"
          }
        },
        "7b8fddabfec64b518082c6f3b5247502": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68dc60728f114f63a2947544f30ef6b5",
            "max": 303,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e66fbc2b4214a8698ccbd9a0ae4319b",
            "value": 303
          }
        },
        "7e66107d38f3447d8b323f4a62ab04c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ed5d75b2fc44161b9c1f241db8e8ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ffa82004db140f692c01965532f19b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_362ab3fa194b4d1e8b96c37994a6b2cd",
              "IPY_MODEL_61c53c66b1e84922982125745e847904",
              "IPY_MODEL_8c5c395c57124de5b79fb38f3779483f"
            ],
            "layout": "IPY_MODEL_90c7355e2cc143f89543b93792d07151"
          }
        },
        "81935f7f67e14a699bdd1801c4062d06": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a2594b088ff4ab89a1abe607b5a63c7",
              "IPY_MODEL_7b8fddabfec64b518082c6f3b5247502",
              "IPY_MODEL_bdc831ab4a8d42d2b1f2d320f63e6cf9"
            ],
            "layout": "IPY_MODEL_cd2cbe2de51b40299c1a8830a7018756"
          }
        },
        "838b2d52e34d47c6beaf672b03a47d81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f2199ef658b74e6f84b6dbba73bfc74f",
            "placeholder": "​",
            "style": "IPY_MODEL_21c7d64071f34c83a0c5f025dbcc52cf",
            "value": " 1947/1970 [00:05&lt;00:00, 411.45 examples/s]"
          }
        },
        "83bab447f258418b8a795d41f507bbea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c84ff8b11ecc48fab046a053072ee766",
            "placeholder": "​",
            "style": "IPY_MODEL_dd4ad12037b744a4aa524c848e1bc7f0",
            "value": "Map:  99%"
          }
        },
        "84837790f4e8464db6ac2819a2173b1c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8496c7c2340c4ababfc5d5002509d2f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0c0c1d5f8934274abd15684fac4a1b2",
            "max": 303,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0247154c67404ca9a84944535b1f4a7d",
            "value": 303
          }
        },
        "84f8ca37318e4846a38a9fd86567fd49": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85fdf22d367847f0b2217f4c17f1ecae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8633274057b3478e93904f761eafa08d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "87fd461cf9104848bd58332cceb09524": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88a5d7780d9242f1bfcd3aa01a7cdf9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d9b0a83816a4fa6b0c1836aae949154",
              "IPY_MODEL_3cd7427d9942487880d36812919c6498",
              "IPY_MODEL_89861bc0db3a4d75bea11dc09a706e00"
            ],
            "layout": "IPY_MODEL_a3eb6ad44b9f4777ba40ba9b318a43dd"
          }
        },
        "89861bc0db3a4d75bea11dc09a706e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be99cfca4780466d81d7543efb817d73",
            "placeholder": "​",
            "style": "IPY_MODEL_15e3d6222ff84981b8c903f8f7a32e69",
            "value": " 2716/2716 [00:06&lt;00:00, 380.30 examples/s]"
          }
        },
        "8c224c24c7ab4ce886cda64a43f2e2f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c5c395c57124de5b79fb38f3779483f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_479dfdbe1ff746aca5bf9c91d322a7d4",
            "placeholder": "​",
            "style": "IPY_MODEL_2db6c5e732764aa88fca6a626c92e779",
            "value": " 465/498 [00:01&lt;00:00, 379.68 examples/s]"
          }
        },
        "8e658aa0ccb44bf6979266f0c8410847": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e9f20ffd5694a9cae404fc1711c1caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "90c7355e2cc143f89543b93792d07151": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "94efdd3ef8e5465ca716ec316c008c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96773e984bd54f6090923d6dc0b6f9cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_462fdfe5256b493e8adaa6ecb55456ea",
            "max": 2716,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f3bb89eecc5b4cb0b12fa4a6d566b40f",
            "value": 2716
          }
        },
        "991ae703c590467e8b4c31c4ee0ce21b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a2594b088ff4ab89a1abe607b5a63c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f82cb297757048d4aa24cfad2b56ba7f",
            "placeholder": "​",
            "style": "IPY_MODEL_eb75a735050e477f8d13d70c12adb7f8",
            "value": "Map:  93%"
          }
        },
        "a058da3d27e142c386397d944aff33ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a3eb6ad44b9f4777ba40ba9b318a43dd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "a70c0d52fb0a4489b55bac7aabaa46cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a70d19e17f13478e91f336fc0ab44ae6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa0c77329b284259a66270ddaf09b4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad0565a3e4be4031a76dcfbeb33ed330": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "b13efa5a940d430aaa7f5176acebb5ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b146db5409e44b29a14da22b2a036a54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b48dc83f2a0c4211bd696f09219550ca": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "ba81ec54a983436c85e4413f99992bb0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "bdc831ab4a8d42d2b1f2d320f63e6cf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9ca1130102543af99bf727245d36f6e",
            "placeholder": "​",
            "style": "IPY_MODEL_0402d6de68554f109dc3d65e19c04b99",
            "value": " 283/303 [00:00&lt;00:00, 468.43 examples/s]"
          }
        },
        "be82587ef1e14137b671a5b6e8b709c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "be99cfca4780466d81d7543efb817d73": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf9fcbecb4fa4627adb40d2e88ded7e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a70d19e17f13478e91f336fc0ab44ae6",
            "placeholder": "​",
            "style": "IPY_MODEL_8e9f20ffd5694a9cae404fc1711c1caf",
            "value": " 65/106 [00:00&lt;00:00, 563.53 examples/s]"
          }
        },
        "c0c0c1d5f8934274abd15684fac4a1b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7c55ef5649849c1a9ef58e77845d9a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c84ff8b11ecc48fab046a053072ee766": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c87ad2ecbd5e48bdb515c52f9a53c885": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb7e1f8ece62420680a4ebab2a92afd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ded6961e5634cd3b845f7f916dd299f",
            "max": 498,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_551a4623c15c403bb36ad0a9ba131535",
            "value": 498
          }
        },
        "cd2cbe2de51b40299c1a8830a7018756": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "d115741e57ac4489aa74eb36453907e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d3a39d9de0cd4233865443fda32e8ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d44a288f184641f08a6b9b788e92f85b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4275c30d66f4e6f93fcb22ccb63858f",
            "placeholder": "​",
            "style": "IPY_MODEL_1a920a25f6a04a15abee42958e5e0993",
            "value": "Map:  99%"
          }
        },
        "d543dba028c6436db9b2dd179b05a10f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d9ca1130102543af99bf727245d36f6e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "daddc964e23e4674bc9dea9a7d9ff915": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd4ad12037b744a4aa524c848e1bc7f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e0203a495e1b4154b7c6f9d63087ad0c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4275c30d66f4e6f93fcb22ccb63858f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e619868d514c4bf4a33f7cb96e13e707": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83bab447f258418b8a795d41f507bbea",
              "IPY_MODEL_cb7e1f8ece62420680a4ebab2a92afd5",
              "IPY_MODEL_31d7cff0b77a407c8b91edbcfb0ab7f8"
            ],
            "layout": "IPY_MODEL_be82587ef1e14137b671a5b6e8b709c5"
          }
        },
        "e68cd4a5423d44429c1ebaaf6b872c9d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8e873fab9cd4ce582d7d6e900aadc25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ede80a63d92c4b15a9257204ba27868b",
              "IPY_MODEL_8496c7c2340c4ababfc5d5002509d2f1",
              "IPY_MODEL_7ade8ac56cd84b51ad560598ff2fd547"
            ],
            "layout": "IPY_MODEL_4405344bef97482292a74304f571f0df"
          }
        },
        "eb400f1926ef437c98aa6abdead92130": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45b45d6f1c0a40dc9bd4e3dad81ab786",
            "max": 469,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c224c24c7ab4ce886cda64a43f2e2f3",
            "value": 469
          }
        },
        "eb75a735050e477f8d13d70c12adb7f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ed96af72f21141a7ae2f3cfdc8999035": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_87fd461cf9104848bd58332cceb09524",
            "placeholder": "​",
            "style": "IPY_MODEL_a058da3d27e142c386397d944aff33ba",
            "value": " 13586/13586 [00:04&lt;00:00, 2994.26 examples/s]"
          }
        },
        "ede80a63d92c4b15a9257204ba27868b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41706078fbbb415284d518cd0abdeb96",
            "placeholder": "​",
            "style": "IPY_MODEL_4bc2997502ad46fdbd6b633f5867a508",
            "value": "Map:  99%"
          }
        },
        "f00ee97110ef4dd19c87d725864208dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f03fd0cccab645588c8214ab5ca34690": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0203a495e1b4154b7c6f9d63087ad0c",
            "max": 13586,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_42742af3f32643a4a3c5199d9accc3a9",
            "value": 13586
          }
        },
        "f2199ef658b74e6f84b6dbba73bfc74f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3bb89eecc5b4cb0b12fa4a6d566b40f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f65da7ad38af40279e1510d46e70c33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fddf94e4f974252ace1480e436b61a6",
              "IPY_MODEL_96773e984bd54f6090923d6dc0b6f9cf",
              "IPY_MODEL_f7076ff5bdc64ac8833d8abf73131156"
            ],
            "layout": "IPY_MODEL_b48dc83f2a0c4211bd696f09219550ca"
          }
        },
        "f7076ff5bdc64ac8833d8abf73131156": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_84837790f4e8464db6ac2819a2173b1c",
            "placeholder": "​",
            "style": "IPY_MODEL_4d872210910a41429be598357ac15801",
            "value": " 2680/2716 [00:07&lt;00:00, 361.48 examples/s]"
          }
        },
        "f82cb297757048d4aa24cfad2b56ba7f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc6c5972c2f640e39aee0064bff2087d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7ed5d75b2fc44161b9c1f241db8e8ecc",
            "placeholder": "​",
            "style": "IPY_MODEL_ff61c11d57964ab29a9b805fe4033546",
            "value": "Map:  99%"
          }
        },
        "ff418fc49cdc46bfbe28eaf149adbb5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff61c11d57964ab29a9b805fe4033546": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
